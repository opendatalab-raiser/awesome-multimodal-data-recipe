# Awesome 多模态数据合成方法 [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<div align="center">
  <img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome">
  <img src="https://img.shields.io/github/stars/opendatalab-raiser/awesome-multimodal-data-recipe?style=flat-square" alt="Stars">
  <img src="https://img.shields.io/github/forks/opendatalab-raiser/awesome-multimodal-data-recipe?style=flat-square" alt="Forks">
  <img src="https://img.shields.io/github/license/opendatalab-raiser/awesome-multimodal-data-recipe?style=flat-square" alt="License">
  <img src="https://img.shields.io/github/last-commit/opendatalab-raiser/awesome-multimodal-data-recipe?style=flat-square" alt="Last Commit">
</div>

<p align="center">
  <a href="README.md">English</a> | <a href="README_zh.md">中文</a>
</p>

<p align="center">
  <b>精选的多模态数据合成方法与资源列表，专注于视觉-语言模型</b>
</p>

---

## 📊 统计信息

- **论文总数：** 30+篇（数据合成/构建方法）
- **大厂报告：** 9篇（百度、微软、阿里巴巴、字节跳动、腾讯等）
- **数据合成方法：** 
  - 图像生成 - 合成新视觉内容(4篇): 几何/数学推理 + 文档/文本密集场景
  - 图像编辑(4篇): 非刚性运动、统一编辑、指称表达式引导编辑
  - 组合性/偏好导向合成(1篇): 增强组合理解能力
  - 交错图文·连贯性与一致性(1篇): 多视角质量过滤
  - 图像介入推理(1篇): 图像主动参与推理过程
  - 图像不变 - 文本增强(16篇): 固定图像，仅丰富文本
- **典型数据集：** 
  - 4个交错图文数据集（OmniCorpus、OBELICS、MMC4、CoMM）
  - 2个领域特定数据集（MMM-RS、MESED）
  - 4个图像编辑数据集（ByteMorph-6M、ImgEdit、RefEdit、RefCOCO-Edit）
  - 4个大规模通用训练数据集
- **开源数据集：** 25+个数据集完全开源

---

## 📋 目录

- [简介](#-简介)
- [大厂与开源项目的数据合成方法](#-大厂与开源项目的数据合成方法)
- [按图像处理方式分类](#-按图像处理方式分类)
  - [图像生成 - 合成新视觉内容](#-图像生成---合成新视觉内容)
    - [几何与数学推理](#-几何与数学推理)
    - [文档/文本密集场景](#-文档文本密集场景)
  - [图像介入推理](#-图像介入推理)
  - [图像编辑（方法+数据）](#-图像编辑方法数据)
  - [组合性/偏好导向合成](#-组合性偏好导向合成)
  - [交错图文·连贯性与一致性](#-交错图文连贯性与一致性)
  - [图像不变文本增强](#图像不变文本增强)
- [跨领域方法论洞察](#-跨领域方法论洞察)
- [典型多模态数据集](#-典型多模态数据集)
  - [交错图文数据集](#-交错图文数据集)
  - [领域特定与知识导向数据集](#-领域特定与知识导向数据集)
  - [大规模通用训练数据集](#-大规模通用训练数据集)
  - [图像编辑数据集](#-图像编辑数据集)
- [基准数据集](#-基准数据集)
- [资源](#-资源)
- [贡献指南](#-贡献指南)

---

## 🎯 简介

多模态数据合成是提升视觉-语言模型(VLMs)性能的关键技术。本仓库收集并整理：

- 🏢 **大厂报告**：来自领先科技公司的详细数据合成pipeline和最佳实践
- 📚 **学术论文**：最先进的研究方法和创新技术
- 🛠️ **工具与框架**：实用的数据合成工具和代码库
- 📊 **数据集**：高质量的多模态数据集

**关键观察**：当前多模态数据合成主要遵循"图像不变+文本增强"范式，利用Web资源、工具API或其他大模型来提升图像-文本对的质量和多样性。

---

## 🏢 大厂与开源项目的数据合成方法

> 本节包含明确记录了数据合成pipeline的工作。**仅收录具有详细数据构建和合成方法描述的项目。**

### 百度 - 千帆-VL

<details>
<summary>点击展开</summary>

**论文**: [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)

**发布时间**: arXiv 2025年9月

**机构**: 百度AI云千帆团队

**📊 数据合成方法（Introduction & Section 3.2）**:

千帆-VL为关键企业场景开发了**comprehensive data synthesis pipelines**，涵盖六大任务类别：

**1. 合成范围**（Introduction明确指出）:
> "Our synthesis covers **six major task categories**: document OCR, mathematical problem-solving, chart understanding, table recognition, formula recognition, and natural scene OCR."

**任务类别**:
- **文档OCR**: 合成文档图像生成及标注
- **数学问题求解**: 自动化数学问题和解答生成
- **图表理解**: 程序化图表生成及QA对
- **表格识别**: 合成表格结构和内容生成
- **公式识别**: 数学公式渲染和标注
- **自然场景OCR**: 场景文字图像合成

**2. 合成方法**（Introduction）:
> "By **combining traditional computer vision models with programmatic generation techniques**, we create high-quality training data at scale."

**关键技术**:
- **传统CV模型**: 利用现有计算机视觉模型进行标注
- **程序化生成**: 使用基于代码的生成方法处理结构化内容
- **领域特定增强**: 针对每种任务类型的定制增强策略
- **质量验证机制**: 自动化质量检查确保数据可靠性

**3. 训练数据规模**（Section 3.1 - 四阶段渐进式训练）:
- 跨模态对齐: 100B tokens
- 通用知识注入: 2.66T tokens
- **领域增强: 0.32T tokens**（合成数据应用于此阶段）
- 指令微调: 1B tokens

**4. 质量保证**（Introduction）:
> "Each pipeline incorporates **domain-specific augmentation strategies and quality verification mechanisms** to ensure data reliability."

**模型变体与能力**:
- **Qianfan-VL-3B**: 32K上下文，优化用于边缘设备和实时OCR
- **Qianfan-VL-8B**: 32K上下文，带思维链，用于服务器和通用应用
- **Qianfan-VL-70B**: 32K上下文，带思维链，用于云端和复杂推理

**实验结果**:
- **OCRBench**: 873分（70B版本）
- **DocVQA**: 94.75%准确率（70B版本）
- **MathVista**: 78.6%分数（70B版本）
- 通用基准强劲表现: CCBench, SEEDBench_IMG, ScienceQA, MMStar

**训练基础设施**:
- 完全在百度**昆仑P800芯片**上训练
- 在5000+芯片集群上实现**>90%扩展效率**
- 验证了专有硬件训练SOTA模型的可行性

**✅ 技术报告**: 
- [arXiv:2509.18189](https://arxiv.org/abs/2509.18189)
- 领域增强多模态模型的综合方法论

**意义**:
- **多领域合成**: 覆盖六大关键企业任务类别
- **混合方法**: 结合CV模型和程序化生成
- **大规模应用**: 0.32T tokens的领域特定合成数据
- **质量优先**: 整个pipeline融入验证机制
- **企业就绪**: 为企业部署场景而设计

</details>

### 阿里巴巴 - Qwen-VL系列

<details>
<summary>点击展开</summary>

**论文**: 
- [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://arxiv.org/abs/2308.12966)

**📊 数据合成方法（Section 3.1.2）**:

Qwen-VL论文在Section 3.1.2描述了grounding数据构建：

1. **Grounding数据合成**（Section 3.1.2）:
   - 利用现有检测数据集（COCO, Objects365）的bbox标注
   - **将bbox坐标转换为归一化文本格式**（例如：`<box>0.1,0.2,0.3,0.4</box>`）
   - **生成指令**: 创建指令-响应对，如"在图像中找到X"
   - 这是一种将结构化标注转换为文本格式的数据合成方法

**⚠️ 注意**: 
- 这属于**数据格式转换和指令生成**，归类为数据合成
- 论文未完全公开prompt模板或生成方法
- 其他部分（数据来源、清洗）主要是数据收集和过滤，非合成

</details>

### 阿里巴巴 - MMEvol（2024）

<details>
<summary>点击展开</summary>

**论文**: [MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct](https://arxiv.org/abs/2409.05840)

**机构**: 阿里巴巴达摩院（Fei Huang, Yongbin Li等）

**📊 数据合成方法（明确描述）**:

MMEvol提出了一种**多模态指令进化框架**，通过迭代提升数据质量和复杂度：

1. **核心方法 - Evol-Instruct范式**（基于文本领域的Evol-Instruct）:
   - 从初始种子数据**SEED-163K**开始
   - **迭代进化**: 通过多轮持续改进指令数据
   - 三个进化维度:
     a) **细粒度感知**: 挖掘图像中的详细信息
     b) **认知推理**: 扩展视觉推理步骤，增强推理能力
     c) **交互进化**: 提升指令类型的多样性

2. **数据进化Pipeline**:
   - 初始指令 → 进化操作 → 更复杂/多样的指令
   - 系统化扩展指令类型多样性
   - 逐步提升视觉推理复杂度
   - 深入探索图像中的细粒度信息

3. **实验结果**（Section 5）:
   - 相比基线模型（使用种子数据），**平均准确率提升3.1个百分点**
   - **在9个任务上达到SOTA**，使用更少数据
   - 在13个视觉-语言任务上全面评估

**数据规模**:
- 初始种子: SEED-163K
- 通过进化生成更多样化和复杂的数据

**✅ 论文**: 
- [arXiv:2409.05840](https://arxiv.org/abs/2409.05840)
- 最新版本: v5（2024年12月31日）

**与Oasis的关系**:
- Oasis论文将MMEvol作为对比方法之一
- MMEvol专注于迭代进化已有数据，而Oasis专注于从单图生成数据
- 两种方法都能提升数据多样性和质量

</details>

### 字节跳动 - Oasis

<details>
<summary>点击展开</summary>

**论文**: [Oasis: One Image is All You Need for Multimodal Instruction Data Synthesis](https://arxiv.org/html/2503.08741v2)

**📊 数据合成方法（Section 3.2详细描述）**:

**这是最新的突破性工作！** Oasis提出了一种极简的数据合成方法，灵感来自Magpie，但应用于多模态领域：

1. **核心创新 - "Hooking" Prompt**（Section 3.2 Step 1）:

   **关键突破**:
   - **只输入图像**，不需要任何文本prompt！
   - 让MLLM（如Qwen2-VL）自己生成instruction
   - 利用模型的自回归特性，基于图像自动生成多样化的问题

   **为什么有效**:
   - 打破传统的固定prompt模式，大幅提升数据多样性
   - 模型根据自己的知识库生成问题，覆盖面更广
   - 简单高效，不需要人工设计复杂的prompt

2. **数据分类**（Section 3.2 Step 2）:
   - 使用LLM过滤掉非指令类数据（如纯描述性文本）
   - 确保生成的都是question-answer格式

3. **四维质量控制**（Section 3.2 Step 3，附录B.2完整公开）:

   论文设计了严格的质量评估标准（1-5分）：

   a) **Solvability（可解性）**: 图像是否包含足够信息回答问题
   b) **Hallucination（幻觉）**: 问题是否与图像内容一致
   c) **Clarity（清晰度）**: 问题表述是否明确
   d) **Nonsense（无意义）**: 问题是否语法正确、逻辑合理

   - 每个维度都有详细的评分标准（附录B.2完整prompt）
   - 多维度综合评估，确保高质量

**数据规模**:
- 生成了500K高质量指令数据
- 在LLaVA-NeXT上验证有效性

**领域定制能力**:
- 由于只依赖图像，可以通过控制图像来源来生成特定领域数据
- 论文展示了OCR领域的案例（Section 4.3）

**实验结果**（Section 4.2）:
- 在14个benchmark上显著提升性能
- 优于其他数据合成方法（包括LLaVA、ALLAVA、MMEvol等）

**Oasis论文中提到的对比方法**（Section 4.2）:
- **LLaVA** [24]: GPT-4辅助生成（见下方"LLaVA系列"条目）
- **MMEvol** [29]: 阿里巴巴达摩院的图像-文本指令进化框架（见上方"阿里巴巴 - MMEvol"条目）
- **ALLaVA** [4]: Captioning-then-QA方式（见下方"按图像处理方式分类"条目）

**Oasis灵感来源**:
- **Magpie** [43]: 文本领域自对齐指令生成方法（启发了Oasis的"hooking prompt"核心思想）
  - Oasis将Magpie的思想扩展到多模态领域，实现了仅用图像的指令生成

**✅ 完全开源**: 
- 论文承诺开源500K数据和模型
- 所有质量控制的prompt都在附录B公开

**重要性**:
- **极简方法论**: 只需要图像，不需要caption或其他文本
- **质量保证**: 四维质量控制非常系统
- **可复现**: 所有prompt和方法都完整公开

</details>

### 微软 - Florence-2

<details>
<summary>点击展开</summary>

**论文**: [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks](https://arxiv.org/abs/2311.06242)

**发布时间**: arXiv 2023年11月

**机构**: Azure AI, Microsoft

**📊 数据合成方法（Introduction & Section 2）**:

Florence-2开发了一个**自动化数据引擎**来生成大规模**FLD-5B**数据集：

1. **数据规模**（Abstract）:
   - **54亿comprehensive visual annotations**
   - 覆盖**1.26亿images**
   - 最大规模的自动标注多模态数据集之一

2. **自动化数据引擎 - 双模块Pipeline**（Introduction，第2页）:

   **模块1 - 协作自动标注**:
   > "uses specialized models to collaboratively and autonomously annotate images, moving away from the traditional single and manual annotation approach. Multiple models work together to reach a consensus"
   
   - 利用多个专门模型协作进行图像标注
   - 模型通过多模型一致性达成共识（受"群体智慧"概念启发）
   - 确保更可靠和无偏的图像理解
   - 完全自动化，消除人工标注需求

   **模块2 - 迭代精炼与过滤**:
   > "iteratively refines and filters these automated annotations using well-trained foundational models"
   
   - 使用训练良好的基础模型迭代精炼和过滤标注
   - **迭代策略**: 模型标注 → 精炼 → 模型重训练 → 再标注循环
   - 持续提升标注质量

3. **标注覆盖范围**（Introduction）:
   FLD-5B数据集涵盖多个视觉任务：
   - **图像captioning**（各种语义粒度）
   - **物体检测**（空间层次理解）
   - **Grounding**（文本-区域对齐）
   - **分割**（细粒度视觉理解）

4. **核心创新**:
   - **完全自动化pipeline**: 无需人工标注
   - **多任务统一格式**: 所有标注转换为文本格式用于统一学习
   - **迭代改进**: 通过模型-标注共同进化持续提升质量
   - **Billion级规模**: 达到前所未有的标注规模

**模型架构**:
- Sequence-to-sequence (seq2seq)结构
- 统一的基于prompt的表示用于多种视觉任务
- 单一模型处理多个任务，无需任务特定架构修改

**实验结果**:
- **Zero-shot SOTA**: 在COCO captioning、Flickr30k visual grounding、RefCOCO/+/g referring expression comprehension上达到新的zero-shot SOTA
- **迁移学习**: 大幅超越ImageNet预训练，在COCO和ADE20K数据集上提升6.9、5.5和5.9个点
- **训练效率**: 比监督预训练快4×

**✅ 开源**: 
- 模型: 在HuggingFace可用
- 代码: [Microsoft Florence](https://github.com/microsoft/Florence)

**意义**:
- **工业规模数据引擎**: 展示了完全自动化大规模标注的可行性
- **多任务合成**: 统一方法生成多种标注类型
- **通过共识保证质量**: 多模型协作确保标注可靠性

</details>

### 腾讯混元 - Bee、HoneyPipe 与 DataStudio

<details>
<summary>点击展开</summary>

**论文**: [Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs](https://arxiv.org/abs/2510.13795)

**发布时间**: arXiv 2025年10月

**机构**: 腾讯混元团队 & 清华大学

**作者**: Yi Zhang, Bolin Ni, Xin-Sheng Chen, Heng-Rui Zhang, Yongming Rao, Houwen Peng, Qinglin Lu, Han Hu, Meng-Hao Guo, Shi-Min Hu

**📊 三大核心贡献**:

**1. Honey-Data-15M**: 1500万样本的SFT数据集，经过精心清洗并enriched with双层思维链推理

**2. HoneyPipe + DataStudio**: 数据筛选pipeline及其底层模块化框架，提供透明且可适应的方法论

**3. Bee-8B**: 8B模型，在完全开源MLLM中建立新SOTA，与半开源模型（如InternVL3.5-8B）竞争

---

**📊 数据合成方法 - HoneyPipe（Section 2）**:

HoneyPipe是一个**自动化且可复现的工作流**，基于**DataStudio的模块化组件**构建，采用精妙的**双层推理增强策略**：大规模短CoT增强的基础路径 + 针对复杂指令的长CoT专用循环。

**🔧 阶段1：数据聚合与准备**
- **初始池**: 约2400万图像-文本对，来自多个社区数据集
  - 来源：LLaVA-OneVision、PixMo、MAmmoTH-VL等
  - 挑战：显著的内容重叠
- **严格去重**（pair级别）:
  - 方法：感知哈希（图像）+ Simhash（指令）
  - 移除标准：图像**和**指令**都**必须相同
  - 目的：最大化多样性，提升处理效率
- **领域标注**: 人工检查和分类
  - 领域：General、Chart、OCR、STEM等
  - 目的：指导后续处理
- **输出**: 带领域标签的清洁、唯一图像-指令-响应三元组

**🔧 阶段2：噪声与无关性过滤**
- **基于规则的算子**（格式问题）:
  - 移除非常小的图像
  - 过滤极端长宽比
  - 消除指令中的重复文本
- **基于模型的过滤算子**（使用**Qwen2.5-VL-72B**）:
  - 评估：指令是否逻辑合理且可回答？
  - 验证：指令与视觉内容是否语义相关？
  - 示例：将"解决这个函数问题"标记为与橙子图像无关
- **结果**: 有效剪枝有缺陷的样本，产生清洁的图像-指令对

**🔧 阶段3：短CoT增强与验证**（基础路径）

*此阶段针对需要中等推理的指令*

**数据分流**:
- CV任务（OCR、物体检测）→ **跳过增强** → 最终数据集
- 其他样本 → 进入CoT增强流程

**短CoT增强**（约1220万样本）:
- **预处理**: 移除抑制推理的提示
  - 移除："直接回答"等头尾提示
  - 目的：激发全面的、逐步的响应
- **生成**: 使用**Qwen2.5-VL-72B/32B**（强大的开源MLLMs）
  - 将简单短响应 → 详细推理路径
  - **无额外系统提示**: 模型已擅长多步骤响应
  - 避免约束：保持输出多样性
- 12.2M短CoT样本的**主要来源**

**保真度验证**（LLM-as-a-Judge）:
- **验证器模型**: Qwen2.5-VL-72B
- **方法**: 新生成CoT最终结论与原始响应之间的语义比较
- **评估标准**（双重）:
  - **事实性查询**（客观）: 最终响应必须**精确**匹配
  - **描述性查询**（主观）: 需要主题相关性和语义一致性
- **通过** → 加入最终数据集
- **失败** → **不丢弃**，路由到长CoT增强循环进行专门增强

**🔧 阶段4：长CoT增强循环**（复杂指令专用路径）

*专为需要深度、多步骤问题解决的最复杂指令设计*

**输入来源**（3种主要类型）:
1. **失败样本**: 阶段3保真度验证失败的样本
2. **特定数据源**: 识别为固有复杂的数据源
   - 例如：VisualWebInstruct
   - 策略：主动生成长CoT**并且**同时生成短CoT
3. **验证数据集**: 先前研究验证的数据集
   - 例如：Vision-R1
   - 标准：特别适合生成深度推理链

**深度推理生成**（约270万样本）:
- **模型**: 利用**顶级专有MLLMs**生成更详细的解决方案
- **过程**: 
  - 首先生成深度推理（通常使用`<think></think>`标签结构化）
  - 然后输出最终响应
- **能力**: 处理初始开源模型无法胜任的复杂指令

**最终保真度验证**:
- 与阶段3相同的验证过程
- **通过** → 构成Honey-Data-15M中约270万长CoT数据点
- **失败** → **丢弃**（假定为错误、无解或标注成本过高）

---

**📦 最终数据集 - Honey-Data-15M**:

**规模**: 1500万精心筛选样本

**组成**（7大领域）:
- General（36.8%）: 基础视觉理解
- Chart（24.6%）: 图表理解与推理
- Caption（15.1%）: 图像描述
- STEM（7.6%）: 符号推理（数学、科学、几何）
- Document（5.9%）: 文档理解与OCR
- Grounding & Counting（5.1%）: 物体检测与计数
- OCR（4.9%）: 各种场景文本识别

**双层CoT主干**:
- **约1220万短CoT样本**: 中等推理的基础性、逐步逻辑推断
- **约270万长CoT样本**: 复杂问题解决的复杂、多步推理，需要更深入的综合
- **针对性方法**: 根据指令复杂度定制响应深度
- **固有解决方案**: 识别哪些指令需要详细的多步骤解决方案

---

**🤖 Bee-8B模型 - 验证与性能**:

**架构**:
- LLM: Qwen3-8B（推理和文本生成）
- 视觉编码器: SigLIP2-so400m-patch14-384
- 分辨率策略: Anyres（处理可变分辨率，保留细粒度细节）
- 投影器: 带GELU激活的两层MLP

**训练**: 5阶段渐进式过程（详见Section 3.2）

**性能亮点**（在完全开源MLLM中建立新SOTA）:

*通用VQA*:
- **MMMU**: 66.8（与半开源模型竞争）
- **MMMU-Pro**: 50.7（**领先Qwen2.5-VL-7B 3.6%**）
- **MMStar**: 71.4
- **MMVet**: 83.9
- **MMVP**: 82.0

*数学与推理*（突出表现）:
- **MathVista mini**: 81.4
- **MathVerse**（vision_only）: 67.0
- **MathVision**: 50.0
- **LogicVista**: 61.3
- **DynaMath**（worst）: 40.5
- **WeMath**: 59.8

*关键观察*: 在**事实准确性**和**复杂多步推理**方面优势最显著，直接反映Honey-Data-15M的优势

**全面消融研究**:
- 量化每个筛选阶段的影响
- 在多个benchmark上显示显著改进
- 证实：关注数据质量 > 竞争数据量
- 证据：数据清洗 + CoT增强至关重要

---

**🎯 核心创新 - 双层CoT策略**:

1. **渐进式增强**: 逐步提升质量而非简单丢弃
2. **失败样本恢复**: 短CoT验证失败的样本获得专门的长CoT增强
3. **模型驱动过程**: MLLM自动化工作流（人工标注的可扩展、经济替代方案）
4. **复杂度识别**: 固有地解决了识别哪些指令需要深度推理的挑战

---

**✅ 开源资源**:
- **项目主页**: https://open-bee.github.io
- **论文**: [arXiv:2510.13795](https://arxiv.org/abs/2510.13795)
- **承诺发布**（完整套件）:
  - Honey-Data-15M语料库
  - HoneyPipe + DataStudio框架
  - 训练recipes
  - 评估工具
  - Bee-8B模型权重

---

**💡 重要意义**:

- **缩小差距**: 证明完全开源MLLM可通过关注数据质量与半开源模型竞争
- **透明方法论**: 超越静态数据集发布，提供不断演进、可适应的筛选方法
- **社区基石**: 为完全开源MLLM社区提供新的基础资源
- **可扩展性**: 模型驱动pipeline使开源社区的高质量数据构建变得可行
- **验证**: Bee-8B的SOTA性能确认数据筛选策略的有效性

**核心论点得到证实**: *对数据质量的原则性关注是开发与半开源模型高度竞争的完全开源MLLM的关键途径*

</details>

### 字节跳动 - ByteMorph

<details>
<summary>点击展开</summary>

**论文**: [ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions](https://arxiv.org/abs/2506.03107)

**发布时间**: arXiv 2025年6月 (v2: 2025年6月)

**机构**: 字节跳动Seed、南加州大学、东京大学、UC伯克利、斯坦福大学、UCLA

**作者**: Di Chang*, Mingdeng Cao*, Yichun Shi, Bo Liu, Shengqu Cai, Shijie Zhou, Weilin Huang, Gordon Wetzstein, Mohammad Soleymani, Peng Wang (*同等贡献)

**📊 数据合成方法 - 自动化运动引导数据引擎**:

ByteMorph解决了指令引导图像编辑中的关键空白：**非刚性运动编辑**。现有数据集主要关注静态编辑（对象操作、风格迁移），而ByteMorph针对涉及相机运动、物体形变和人体关节运动的动态变换。

**1. 核心创新 - 运动引导的分层合成Pipeline**

**四大运动类别**（系统定义）:
- **相机运动 (CM)**: 缩放进/出、透视变换、平移、倾斜、旋转
- **物体形变 (OD)**: 拉伸、弯曲、挤压、扭曲
- **人体关节运动 (HA)**: 身体姿态变化、面部表情
- **人物-物体交互 (HOI)**: 抓取、使用、操作物体

**自动化数据引擎组件**:

**阶段1 - 视频源收集与过滤**:
- **来源**: 网络规模视频语料库
- **过滤标准**: 
  - 高分辨率（≥720p）
  - 前景对象清晰的稳定场景
  - 最小模糊和伪影
- **运动检测**: 光流分析识别运动丰富片段
- **结果**: 精选展现目标运动类型的视频片段

**阶段2 - 分层合成与运动迁移**:
- **方法**: 受专业VFX工作流程启发
- **Pipeline**:
  1. 从过滤视频中提取连续帧
  2. **前景分割**: 使用分割模型隔离运动对象/主体
  3. **背景稳定**: 对背景应用运动补偿
  4. **图层合成**: 将前景叠加到新背景上并保留运动
  5. **质量控制**: 过滤有可见伪影的合成图
- **关键优势**: 大规模保留真实运动动态，同时实现多样化背景组合
- **创新**: 自动化传统手工合成流程

**阶段3 - GPT-4o辅助标注生成**:
- **模型**: GPT-4o（多模态理解）
- **输入**: 源图像 + 编辑后图像对
- **输出生成**:
  - **详细图像描述**: 对两幅图像的全面场景理解
  - **运动感知指令**: 描述运动变换的自然语言命令
    - 示例："让人举起右臂"、"放大猫咪"、"将树向左弯曲"
  - **一致性验证**: GPT-4o验证指令-图像对齐
- **提示工程**: 强调运动语义和空间关系的自定义提示
- **规模**: 处理600万图像对

**阶段4 - 质量保证与过滤**:
- **多维度过滤**:
  - **视觉质量**: 基于CLIP的美学评分
  - **运动连贯性**: 光流一致性检查
  - **指令相关性**: 基于LLM的指令-编辑对齐验证
  - **多样性指标**: 聚类以确保跨运动类型和场景类别的覆盖
- **人工验证**: 对5K随机选择的三元组进行样本质量审核
- **阈值校准**: 过滤阈值的迭代优化

**2. ByteMorph-6M数据集特性**

**规模与构成**:
- **总规模**: 600万高分辨率图像编辑三元组
- **格式**: (源图像, 自然语言指令, 编辑后图像)
- **分辨率**: 最小512×512，大部分1024×1024
- **分布**:
  - 相机运动: ~180万样本
  - 物体形变: ~150万样本
  - 人体关节运动: ~180万样本
  - 人物-物体交互: ~90万样本

**质量属性**:
- **指令多样性**: 每条指令平均15.3个独特单词
- **运动粒度**: 粗粒度（如"放大"）和细粒度（如"相机向左倾斜15度"）指令
- **背景多样性**: 50万+独特背景场景
- **对象类别**: 50+语义类别中的2000+对象类型

**3. ByteMorph-Bench - 评估基准**

**基准设计**:
- **规模**: 613个精心筛选的测试样本
- **难度级别**: 简单、中等、困难（基于运动复杂度）
- **覆盖范围**: 四大运动类别的平衡分布
- **标注**: 专家验证的真实编辑
- **指标**: CLIP相似度、运动准确度（通过光流）、人工评估分数

**挑战性方面**:
- 复杂的多步骤运动（如"人挥手的同时转头"）
- 细粒度关节运动（如特定手指位置）
- 物理感知的形变（如真实的布料弯曲）
- 上下文交互（如"人从桌上拿起杯子"）

**4. ByteMorpher模型 - 扩散Transformer基线**

**架构**:
- **基础**: 扩散Transformer (DiT) 架构
- **条件化**: 对文本指令和源图像的多模态条件化
- **训练策略**:
  - 在ByteMorph-6M（完整数据集）上预训练
  - 在ByteMorph-Bench训练集上微调
- **推理**: 无分类器引导以增强指令遵循

**性能亮点**:
- **ByteMorph-Bench**: 在运动相关指标上超越现有指令引导编辑模型（InstructPix2Pix、MagicBrush）**18.3%**
- **泛化能力**: 成功迁移到域外运动编辑任务
- **人工评估**: 在成对比较中**73.5%**优于基线

**5. 实验结果与洞察**

**关键发现**:
- **运动特定训练至关重要**: 在静态编辑数据集上训练的模型在运动任务上失败（平均成功率32.1%）
- **规模重要性**: 性能随数据集规模对数增长（1M→3M→6M）
- **分层合成优势**: 超越基于扩散的合成（+12.7%真实感得分）
- **GPT-4o标注质量**: 人工评估显示91.2%的指令准确率 vs. 自动化VLM标注的67.4%

**基准比较**:
| 模型 | CLIP-Sim ↑ | 运动准确度 ↑ | 人工偏好 ↑ |
|-------|------------|--------------|--------------|
| InstructPix2Pix | 0.652 | 0.423 | 21.3% |
| MagicBrush | 0.681 | 0.457 | 24.8% |
| **ByteMorpher** | **0.743** | **0.612** | **73.5%** |

**✅ 开源资源**:
- **论文**: [arXiv:2506.03107](https://arxiv.org/abs/2506.03107)
- **数据集**: ByteMorph-6M (600万三元组) - 计划在OpenDataLab发布
- **基准**: ByteMorph-Bench (613测试样本) - 随论文提供
- **模型**: ByteMorpher检查点 - 承诺HuggingFace发布
- **代码**: 数据引擎pipeline代码 - 承诺GitHub发布

**💡 重要意义**:

- **新颖问题表述**: 首个大规模数据集解决指令引导编辑中的非刚性运动
- **工业规模自动化**: 展示可扩展至百万级样本的分层合成方法
- **运动感知Pipeline**: 将运动检测和保留引入自动化数据合成
- **基准贡献**: ByteMorph-Bench填补了评估运动编辑能力的关键空白
- **多模态LLM集成**: 展示GPT-4o在运动语义标注生成中的有效使用
- **开放科学**: 承诺开源数据集、基准、模型和代码

**研究影响**: ByteMorph将指令引导图像编辑的范围从静态变换扩展到基于动态运动的编辑，在视频编辑、动画和增强现实中实现新应用。

</details>

### 字节跳动 & NTU - LLaVA-OneVision

<details>
<summary>点击展开</summary>

**论文**: [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326)

**发布时间**: arXiv 2024年8月 (v3: 2024年10月)

**机构**: 字节跳动、南洋理工大学S-Lab、香港中文大学、香港科技大学

**作者**: Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li

**📊 数据构建方法（Section 4 - 综合数据策略）**:

LLaVA-OneVision代表了一种重要的以数据为中心的方法，在多模态训练中强调**"质量重于数量"**。论文整合了LLaVA-NeXT博客系列的见解，配合精心筛选的大规模数据集（2024年1月至6月积累）。

---

**🔬 高质量知识学习（470万样本）**

*核心原则*: 在有限计算预算下，专注于高质量知识学习而非网络规模的低质量数据。

**关键洞察**: **99.8%的高质量知识数据是合成的**，这是由于收集大规模高质量野生数据的高成本和版权限制。

**三大数据类别**:

**1. 重新描述的详细描述数据（350万样本）**
- **方法**: **自我改进AI**方法
- **模型**: 使用**LLaVA-NeXT-34B**（以强大的详细描述能力著称）
- **过程**: 为现有图像生成新的详细captions
- **来源**: COCO118K, BLIP558K, CC3M
- **创新**: 训练数据由模型早期版本自己生成

**2. 文档/OCR数据（110万样本）**
- **文本阅读子集**: UReader数据集（10万），通过PDF渲染轻松访问
- **合成数据**: SynDOG EN/CN用于文档理解
- **目的**: 增强文本阅读和文档理解能力

**3. 中文和语言数据（23.5万样本）**
- **中文Caption生成（9.2万）**:
  - 图像：原始ShareGPT4V图像
  - 模型：**GPT-4V（Azure API）**生成详细中文描述
  - 目标：提升中文能力
- **语言平衡（14.3万）**:
  - 来源：Evo-Instruct数据集
  - 目的：在视觉描述之外平衡语言理解能力

---

**📦 视觉指令微调数据（480万样本）**

*目标*: 使LMM能够理解并执行跨不同场景的视觉指令

**数据收集策略**:

**分类框架**（三层层次结构）:

1. **视觉输入**: 单图像 / 多图像 / 视频
2. **语言指令**: 通用QA、通用OCR、文档/图表/屏幕、数学推理、语言
3. **语言响应**: 自由形式（GPT-4V/o、Gemini标注）vs 固定形式（学术数据集）

**筛选过程**:
- 从各种原始来源收集，初始比例不平衡
- 纳入Cauldron和Cambrian集合的新子集
- **人工审查和格式化**:
  - 修正问答格式
  - 遵循LLaVA-1.5的prompting策略（多选、简答、OCR）
  - 防止不同数据源的冲突
  - 引导模型行为：平衡QA性能、对话能力、推理技能

**数据组成**:

**单图像数据（320万样本）** - 五大类别:
- **通用（36.1%）**: 70+数据集，包括ALLaVAInst、AOKVQA、Cambrian、LLaVA-158K、ShareGPT4V/4o、VisionFLAN等
- **文档/图表/屏幕（20.6%）**: AI2D、ChartQA、DocVQA、UReader系列、Chart2Text等
- **数学/推理（20.1%）**: MAVIS系列、Geo170K、GeoQA+、GeoMVerse、MathV360K等
- **通用OCR（8.9%）**: ChromeWriting、HME100K、OCR-VQA、SynthDog-EN、TextCaps、TextOCR等
- **语言（14.3%）**: MagpiePro（L3MT、L3ST、Qwen2ST）- 总计45万样本

**OneVision混合数据（160万样本）** - 三种场景:
- **单图像（31.2%，约50万）**: 从之前单图像数据中高质量采样
  - MagpiePro、VisionFLAN、ImageTextualization、Cauldron、UReader、ShareGPT4V/4o等
- **多图像（43.0%，约68.8万）**: 30+数据集
  - NLVR、Co-Instruct、ScanNet、RAVEN、IconQA、VIST、ContrastiveCaption等
- **视频（25.9%，约41.5万）**: 6个数据集
  - ShareGPT4Video（25.5万）、Youcook2、ActivityNet、Charades、NextQA、Ego4D

---

**🎯 核心创新与洞察**:

1. **合成数据主导**: 99.8%的知识数据是模型生成的
   - 成本更低，更容易扩展
   - "随着AI模型越来越强大，从大规模合成数据学习正在成为趋势"

2. **自我改进AI**: 使用LLaVA-NeXT-34B为下一代生成训练数据
   - 为350万图像重新生成更详细的描述
   - 证明从自己模型输出引导的可行性

3. **精心的数据平衡**:
   - 任务分类以维持技能分布
   - 人工审查以防止数据源冲突
   - 跨异构来源的统一prompting策略

4. **跨场景迁移设计**:
   - 洞察：更强的图像模型 → 更好地迁移到多图像/视频任务
   - 训练策略：先单图像，再混合场景
   - 数据分配：为单图像分配更多token以模拟视频表示

5. **质量重于数量理念**:
   - 专注于筛选而非体量
   - 认可预训练LLM/ViT的知识库
   - 持续接触新的高质量数据

---

**📈 训练策略（Section 5）**:

**三阶段课程学习**:
- **Stage-1**: 语言-图像对齐
- **Stage-1.5**: 高质量知识学习（使用470万合成数据）
- **Stage-2**: 单图像指令微调（320万样本）
- **Stage-3**: OneVision混合训练（160万样本）

**模型架构**:
- LLM: Qwen-2（强大的语言能力）
- 视觉编码器: SigLIP（开源编码器中性能更高）
- 投影器: 2层MLP
- 视觉表示: Higher AnyRes策略配合双线性插值

---

**✅ 性能与成就**:

- **首个单一开源模型**在三种场景中同时推动性能边界:
  - 单图像
  - 多图像
  - 视频理解
- 跨模态的强大任务迁移能力
- 通过跨场景迁移展示新兴能力
- 通过从图像迁移任务实现视频理解

---

**✅ 开源资源**:
- **论文**: [arXiv:2408.03326](https://arxiv.org/abs/2408.03326)
- **项目**: https://llava-vl.github.io/blog/llava-onevision
- **数据集**: [🤗 HuggingFace](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data)
- **模型**: 模型检查点已发布
- **代码**: 代码库开源
- **Demo**: 可用的视觉聊天演示

---

**💡 重要意义**:

- **以数据为中心的方法**: 将LLaVA-NeXT博客系列的洞察整合为综合数据集
- **合成数据验证**: 证明大规模合成数据（99.8%）可以达到SOTA性能
- **自我改进路径**: 展示使用自己模型输出进行下一代训练
- **统一多场景**: 单一模型在图像/多图像/视频中表现出色，无需权衡
- **社区影响**: 完整开源数据、模型、代码，实现可复现性

**核心论点**: *通过数据筛选中的质量重于数量，结合战略性使用合成数据和自我改进，能够构建与专有模型竞争的多功能开源LMM。*

</details>

### LLaVA系列（威斯康星大学麦迪逊分校 & 微软）

<details>
<summary>点击展开</summary>

**论文**: 
- [Visual Instruction Tuning (LLaVA)](https://arxiv.org/abs/2304.08485)
- [Improved Baselines with Visual Instruction Tuning (LLaVA-1.5)](https://arxiv.org/abs/2310.03744)

**📊 数据合成方法（Section 3详细描述）**:

**多模态数据合成领域最具影响力的工作之一！** LLaVA论文Section 3 "GPT-assisted Visual Instruction Data Generation"提供了完整pipeline细节：

1. **数据生成Pipeline**（图2展示完整流程）:
   
   **输入材料**:
   - COCO图像
   - COCO人工标注captions（每张图5个）
   - COCO bbox标注

   **使用GPT-4生成三类数据**:
   
   a) **Conversation**（多轮对话，58K）:
   - Prompt模板: 附录A.2.1提供完整prompt
   - 输入: Caption列表
   - 输出: 关于图像内容的多轮Q&A

   b) **Detailed Description**（详细描述，23K）:
   - Prompt模板: 附录A.2.2
   - 要求GPT-4生成比原始captions更详细的描述

   c) **Complex Reasoning**（复杂推理，77K）:
   - Prompt模板: 附录A.2.3
   - 基于bbox，生成需要推理的问题（如计数、空间关系）

2. **完整数据规模**:
   - 总计: 158K指令-响应对
   - 基于约80K COCO图像

3. **Prompt工程细节**（附录完整公开）:
   - 附录A完整公开所有prompt模板
   - 包含system prompts、few-shot示例
   - 完全可复现

**LLaVA-1.5数据改进**:
- 添加更多学术任务数据集（VQAv2, GQA, OKVQA等）
- 增强数据多样性
- 保持相同生成方法

**✅ 数据完全开源**: 
- [HuggingFace Dataset](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)
- Prompt模板直接可用

**影响**: 
这种方法几乎被所有后续开源VLM项目采用或改进（ShareGPT4V、SVIT、InternVL等都基于此范式）。

</details>

---

## 📂 按图像处理方式分类

### 🎨 图像生成 - 合成新视觉内容

该类别专注于**从零生成新图像**作为数据合成pipeline的一部分。这些方法通过程序化或生成模型创建合成视觉内容（几何图表、数学图形、文本密集场景等），并配对相应的文本标注。

#### 📐 几何与数学推理

- **📄 R-CoT** [(OpenReview ICLR 2025)](https://openreview.net/pdf?id=iwVkB9zaVb)
  - **数据合成方法** - **几何推理的逆向思维链**：
    - **核心创新**：结合**引擎准确性**与**LLM多样性**的两阶段几何问题生成管道
    - **阶段1 - GeoChain**：**生成高保真几何图像**及其对应描述
      - 使用**基于代码的引擎**逐步生成准确的几何图像
      - 生成详细描述，突出**几何元素间的关系**
      - 图像保真度高于现有合成几何数据
    - **阶段2 - Reverse A&Q**：从推理结果逆向生成Q&A对
      - **步骤1**：描述块推理 - 从描述进行单步推理
      - **步骤2**：思维链融合 - 逐步将单步推理融合为多步推理
      - **步骤3**：问题生成 - 从推理结果**逆向生成**问题
      - 避免了直接使用LMM从图像生成Q&A时的准确性问题
    - **核心优势**：先生成答案**再生成**问题，减少错误答案
  - **数据规模**：创建GeoMM数据集，包含高保真几何图像和多样化Q&A对
  - **实验结果**： 
    - R-CoT-8B在MathVista上超越之前的开源SOTA模型**16.6%**，在GeoQA上超越**9.2%**
    - 在两个数据集上平均超越GPT-4o **13%**
    - 在2B、7B和8B设置下均达到几何推理新SOTA
  - **发布时间**：arXiv 2024年10月
  - **机构**：百度公司、华中科技大学
  - **开源**：✅ [代码和模型](https://github.com/dle666/r-cot)

- **📄 MAVIS** [(arxiv 2407.08739)](https://arxiv.org/abs/2407.08739)
  - **数据合成方法** - **全自动数学视觉数据引擎**：
    - **核心创新**：完全自动化的基于规则的数据引擎，**独立于人工干预或GPT API使用**
    - **完整数据生成管道**：
      - **图表绘制**：**自动创建数学图表**（图像生成）
      - **标题生成**：生成图表-标题对
      - **Q&A合成**：创建问答对
      - **CoT推理生成**：产生思维链推理
    - **关键特性**：
      - 通过自动规则确保**图表-标题对应性**
      - 通过引擎生成保证**问答正确性**
      - 保持**CoT推理质量**而不依赖专有模型
    - **渐进式4阶段训练管道**：
      - 阶段1：使用MAVIS-Caption微调**CLIP-Math**（数学专用视觉编码器）
      - 阶段2：将CLIP-Math与LLM对齐
      - 阶段3：使用MAVIS-Instruct进行指令调优
      - 阶段4：DPO增强CoT能力
  - **数据规模**： 
    - **MAVIS-Caption**：558K图表-标题对用于视觉-语言对齐
    - **MAVIS-Instruct**：834K视觉数学问题，含详细CoT推理
  - **实验结果**： 
    - MAVIS-7B超越其他7B模型**+9.3%**
    - 超越LLaVA-NeXT-110B **+6.9%**
    - 在数学基准测试中取得开源MLLM领先成果
  - **发布时间**：arXiv 2024年7月
  - **机构**：CUHK、北京大学、上海AI实验室、字节跳动、Oracle
  - **开源**：✅ [代码和数据](https://github.com/ZrrSkywalker/MAVIS)

- **📄 ShareGPT-4o-Image** [(arxiv 2506.18095)](https://arxiv.org/abs/2506.18095) *[同时包含图像编辑]*
  - **数据合成方法** - **蒸馏GPT-4o图像生成能力**：
    - **核心创新**：首个从**GPT-4o图像生成能力**蒸馏的数据集，涵盖文生图和文图混合生图任务
    - **文生图数据生成**（45K对）- **图像生成**：
      - **Prompt优先Pipeline**： 
        - 定义6维属性空间（物体、背景、风格、相机角度等）
        - 采样属性并使用LLM（Gemini-Pro-2.5）组合自然语言提示
        - 将提示传递给**GPT-4o-Image**生成**新配对图像**
        - 确保可控的多样性和复杂性
      - **图像优先Pipeline**：
        - 从ALLaVA数据集获取高质量图像
        - 使用LLM从图像生成详细描述性提示
        - 捕获描述真实场景所需的自然语言
    - **指令引导的图像编辑数据**（46K三元组）- **图像编辑**：
      - 定义**14种图像编辑任务**的分类法，跨5个类别（物体操作、风格迁移、条件控制等）
      - 对于每个（源图像、编辑任务）：LLM合成具体自然语言指令
      - **GPT-4o-Image**执行指令产生编辑输出
      - 创建（源图像、指令、编辑图像）三元组
    - **关键特性**：
      - 完全使用**GPT-4o**能力合成（无人工标注）
      - 涵盖广泛风格和基础视觉推理
      - 体现GPT-4o在指令遵循和视觉美学方面的优势
  - **数据规模**：**91K总计**（45K文生图 + 46K指令引导编辑）
  - **实验结果**： 
    - Janus-4o（在此数据上训练）在EvalGen上比Janus-Pro提升**+4分**，在DPG-Bench上提升**+1.6分**
    - 仅用**91K样本**和**6小时训练**（8×A800）即从零开始实现出色的文图混合生图能力
    - 人工评估显示对Janus-4o输出的强烈偏好
  - **发布时间**：arXiv 2025年6月
  - **机构**：香港中文大学（深圳）
  - **开源**：✅ [代码和数据](https://github.com/FreedomIntelligence/ShareGPT-4o-Image)

#### 📄 文档/文本密集场景

- **📄 TextSSR** [(arXiv 2412.01137)](https://arxiv.org/abs/2412.01137) 🏷️ **[方法 + 数据]** - **ICCV 2025**
  - **聚焦**: **基于扩散的场景文本识别数据合成** - 为文本in-the-wild识别生成训练数据
  - **数据合成方法** - **三大支柱扩散Pipeline：准确性、真实性、可扩展性**:
    - **核心创新**: 端到端扩散式合成，解决渲染方法（缺乏真实感）和纯生成方法（缺乏控制）的局限
    - **支柱1：准确性 - 区域中心文本生成 + 位置-字形增强**:
      1. **区域中心生成**:
         - 与图像级文本生成（易产生幻觉）不同，在**指定边界框内**生成文本
         - 使用**区域条件扩散**（受layout-to-image方法启发）
         - 确保对文本位置的精确控制，防止其他地方出现非预期文本
      2. **位置-字形增强 (PGE)**:
         - **问题**: 扩散模型在准确字符生成和序列顺序方面存在困难
         - **解决方案**: 双流条件化
           - **字形流**: 将目标文本渲染为干净的字形图像（字符形状）
           - **位置流**: 编码边界框内的字符位置
         - **融合**: 通过交叉注意力将字形+位置信息注入扩散UNet
         - **结果**: 保持自然外观的同时实现字符级准确性
    - **支柱2：真实性 - 上下文提示实现自然风格化**:
      - **挑战**: 纯文本控制生成产生通用风格；从真实图像直接风格迁移导致过拟合
      - **方法 - 上下文提示机制**:
        1. **真实场景采样**: 从STR数据集（如COCO-Text、MLT）选择真实场景文本图像
        2. **文本修复**: 使用修复模型从选定图像中移除原始文本
        3. **提示提取**: 从原始文本区域提取**低分辨率上下文提示**（颜色、纹理、方向、退化模式）
        4. **条件生成**: 条件扩散模型基于:
           - 目标文本内容（通过PGE）
           - 边界框位置
           - **上下文提示**（低分辨率纹理/颜色引导）
        5. **结果**: 生成的文本继承真实风格（失真、模糊、光照、透视）而不记忆特定实例
    - **支柱3：可扩展性 - 组合文本排列**:
      - **策略**: 系统性地从词汇表采样创建多样化文本组合
      - **语料来源**:
        - **单词**: 频率列表中的常见英文/中文单词
        - **命名实体**: 人名、品牌、地点
        - **数字序列**: 电话号码、日期、价格
      - **排列**: 生成指定长度内所有可行的n-gram和短语
      - **规模**: 从有限词汇表生成**数百万独特文本实例**
    - **质量筛选**:
      - **后合成过滤**: 使用预训练STR模型验证生成文本与目标标签匹配
      - **多模型共识**: 要求多个STR模型（CRNN、ASTER、ABINet）达成一致
      - **接受率**: ~60-70%的生成图像通过质量筛选
  - **数据规模**: 
    - **TextSSR-F（过滤数据集）**: 355万质量筛选的合成场景文本图像
    - **文本长度**: 1-25字符，多样化分布
    - **风格**: 50+场景类型（街道标识、店面、文档、产品包装等）
    - **语言**: 主要英文 + 多语言子集
  - **训练Pipeline**:
    - **阶段1**: 在真实STR数据集上预训练扩散模型学习场景文本分布
    - **阶段2**: 使用PGE + 上下文提示微调实现准确可控生成
    - **阶段3**: 使用文本排列进行大规模合成
    - **阶段4**: 质量过滤产生TextSSR-F
  - **实验结果**: 
    - **STR基准**: 在TextSSR-F上训练达到**与真实数据训练相当或更优**的性能
      - **IIIT5K**: 95.3%准确率（与真实数据持平）
      - **SVT**: 93.1% (+1.8% 超越合成基线)
      - **ICDAR数据集**: 持续优于基于渲染的合成数据
    - **真实+合成混合**: 通过结合TextSSR-F与真实数据达到最佳结果（平均+2.1%增益）
    - **零样本迁移**: 对未见字体、语言、失真的强泛化能力
  - **消融研究**:
    - **PGE贡献**: +8.3%准确率 超越基线扩散
    - **上下文提示**: +5.7% 超越仅字形条件化
    - **质量过滤**: 提升下游STR准确率4.2%
  - **发布时间**: ICCV 2025 | arXiv 2025年5月
  - **机构**: 未明确说明（学术研究）
  - **开源**: ✅ TextSSR-F数据集（355万图像）、代码、预训练扩散模型 - 论文中提供发布详情
  - **重要意义**: 
    - **弥合渲染-生成差距**: 结合渲染方法的控制性与生成模型的真实性
    - **解决STR数据稀缺**: 为低资源场景实现无限多样化训练数据生成
    - **可泛化框架**: 上下文提示机制可应用于其他条件生成任务
    - **实用影响**: 降低STR标注成本的同时保持或提升模型性能

---

### 💭 图像介入推理

该类别构建**图文交错的推理轨迹**，其中图像在推理过程中**主动介入**并被操作。与传统的纯文本方法不同，这些方法将文本和图像视为**互补模态**，通过渐进式视觉修改（如高亮、叠加、缩放、修复）共同推进问题解决。图像不仅是输入，更是**推理过程的参与者**。

- **📄 ThinkMorph** [(arxiv 2510.27492)](https://arxiv.org/abs/2510.27492) 🏷️ **[Think with Image]**
  - **数据合成方法** - **图像介入的多模态推理**:
    - **核心创新**: 构建**图文交错推理轨迹**，其中图像**主动介入推理过程**，文本和图像作为**互补模态**共同推进问题解决
    - **数据规模**: 约24K高质量交错推理轨迹，涵盖4个任务
    - **任务覆盖**（不同视觉参与度）:
      - **拼图组装** (6K): 可视化重新排列的拼图片段 → 整体空间上下文
      - **空间导航** (6K): 用红线/箭头高亮路径的迷宫叠加
      - **视觉搜索** (6,990): 在目标对象周围绘制边界框
      - **图表重聚焦** (6K): 用红色边界框或叠加层高亮区域
    - **数据合成Pipeline**:
      - **设计原则**: 渐进式 文本→图像→文本 序列
      - **生成策略**: 
        - 拼图&导航: 定制合成pipeline（新生成）
        - 视觉搜索&图表: 从现有数据集通过Human-in-the-loop MLLM过滤
      - **质量控制**: 严格过滤（例如视觉搜索要求目标对象占图像1-30%面积，从144K筛选到6,990高质量样本）
    - **推理模式**: 初始文本建立上下文 → 视觉token操作/可视化 → 最终文本验证解决方案
    - **关键发现**: 交错推理在视觉中心任务上平均优于纯文本和纯视觉5.33%；展现涌现特性，包括未见过的视觉操作（放大、修复、多框生成）
  - **发布时间**: arXiv 2025年10月
  - **机构**: 新加坡国立大学、浙江大学、华盛顿大学、斯坦福大学、香港中文大学
  - **开源**: ✅ [代码&模型](https://github.com/ThinkMorph/ThinkMorph) | [数据集](https://huggingface.co/ThinkMorph)

---

### ✂️ 图像编辑（方法+数据）

该类别专注于**指令引导的图像编辑**，模型学习基于自然语言指令转换图像。这些工作典型地结合**方法创新**（新颖的编辑pipeline、架构）与**大规模数据合成**（自动化数据引擎、质量基准），使其区别于纯数据集构建工作。

- **📄 ByteMorph** [(arXiv 2506.03107)](https://arxiv.org/abs/2506.03107) 🏷️ **[方法 + 数据 + 基准]**
  - **聚焦**: **非刚性运动编辑** - 首个大规模数据集解决相机运动、物体形变、人体关节运动和人物-物体交互
  - **数据合成方法** - **运动引导的分层合成**:
    - **核心创新**: 结合**VFX启发的分层合成**与**GPT-4o运动语义标注**的自动化pipeline
    - **4阶段Pipeline**:
      1. **视频源收集**: 网络规模视频语料库 → 运动检测（光流） → 高分辨率（≥720p）运动丰富片段
      2. **分层合成**: 前景分割 → 背景稳定 → 保留运动的层合成
      3. **GPT-4o标注生成**: 多模态理解 → 运动感知的自然语言指令
      4. **质量保证**: CLIP美学评分 + 光流连贯性 + LLM指令-编辑对齐验证
    - **关键优势**: 大规模保留真实运动动态（自动化手工VFX工作流）
  - **数据规模**: 
    - **ByteMorph-6M**: 600万高分辨率编辑三元组（源图像、指令、编辑图像）
    - **分布**: 180万相机运动、150万物体形变、180万人体关节运动、90万HOI
    - **分辨率**: 大部分1024×1024，最小512×512
  - **基准**: ByteMorph-Bench（613个专家验证测试样本，难度分级）
  - **模型**: ByteMorpher（扩散Transformer基线）
  - **实验结果**: 
    - 在运动指标上超越InstructPix2Pix/MagicBrush **+18.3%**
    - 人工偏好: **73.5%** vs. 基线
    - 证明运动特定训练至关重要（静态数据集模型: 32.1%成功率）
  - **发布时间**: arXiv 2025年6月
  - **机构**: 字节跳动Seed、USC、东京大学、UC伯克利、斯坦福、UCLA
  - **开源**: ✅ 数据集（600万三元组）、基准（613样本）、模型、代码 - 计划在OpenDataLab/HuggingFace发布

- **📄 ImgEdit** [(arXiv 2505.20275)](https://arxiv.org/abs/2505.20275) 🏷️ **[方法 + 数据 + 基准]**
  - **聚焦**: **统一图像编辑数据集和基准** - 涵盖多样化单轮编辑和具有身份一致性的挑战性多轮任务
  - **数据合成方法** - **集成VLM、检测、分割和修复的多阶段自动化Pipeline**:
    - **核心创新**: 结合**VLM编排**与**专用视觉工具**的端到端自动化工作流，实现可扩展的高质量编辑数据生成
    - **5阶段统一Pipeline**:
      1. **初始候选生成（VLM驱动）**:
         - 使用**GPT-4o**分析源图像并生成编辑任务候选
         - 涵盖8大编辑类别: 对象添加、删除、替换、属性修改、背景变化、风格迁移、空间重排、多对象编辑
         - 生成针对图像内容定制的自然语言编辑指令
      2. **定位与检测**:
         - 部署**Grounding DINO** / **YOLO-World**进行对象定位
         - 将VLM识别的编辑目标转换为精确边界框
         - 处理显式对象和抽象概念（如"最左边的椅子"）
      3. **实例分割**:
         - 应用**SAM（Segment Anything Model）**获取像素级精确掩码
         - 确保编辑操作尊重对象边界
         - 对添加/替换任务中的自然合成至关重要
      4. **任务特定修复与编辑**:
         - **对象删除**: 内容感知修复（SD-Inpaint、LaMa）
         - **对象添加**: 基于扩散的对象插入与协调
         - **替换**: 保留上下文的掩码引导生成
         - **属性修改**: 局部风格/颜色调整
         - **背景变化**: 前景保留 + 背景合成
      5. **质量控制与后处理**:
         - **自动过滤**: 基于CLIP的指令-图像对齐评分
         - **伪影检测**: 识别可见接缝、不自然边界、不一致光照
         - **人工回环验证**: 基于样本的质量审核（数据集的5%）
         - **身份一致性验证**（多轮编辑）: 确保相同对象在轮次间保持视觉身份
    - **关键创新**: 与纯生成方法不同，ImgEdit将**复杂编辑分解为模块化视觉任务**，确保可控性和准确性
  - **数据规模**: 
    - **ImgEdit数据集**: 120万高质量编辑三元组（源、指令、编辑后）
    - **单轮编辑**: 95万样本（新颖复杂的单步变换）
    - **多轮编辑**: 25万样本（具有身份一致性的序列编辑）
    - **指令复杂度**: 平均每条指令18.7词，78.3%需要空间/语义推理
  - **基准**: **ImgEdit-Bench**
    - **规模**: 1,000个精选测试样本，涵盖所有编辑类别
    - **难度分层**: 简单（20%）、中等（50%）、困难（30%）
    - **评估指标**: CLIP-Sim、FID、LPIPS、身份一致性分数（多轮）、人工评估
    - **挑战性案例**: 细粒度属性（如"将领带改为条纹图案"）、多对象协调、跨轮次风格保留
  - **实验结果**: 
    - 最先进的指令遵循编辑性能
    - **多轮编辑**: ImgEdit训练模型保持**87.3%身份一致性** vs. 基线的**52.1%**
    - **泛化**: 强迁移到未见编辑类型和领域
    - **人工评估**: 在比较中**69.8%**优于InstructPix2Pix、MagicBrush
  - **发布时间**: arXiv 2025年2月 (v1: 2025年2月)
  - **机构**: 北京大学深圳研究生院、鹏城实验室、Rabbitpre AI
  - **开源**: ✅ 数据集（120万三元组）、基准（1K样本）、代码 - 论文中提供发布详情
  - **交叉引用**: 另见[典型多模态数据集](#-典型多模态数据集)了解数据集详情

- **📄 RefEdit** [(arXiv 2506.03481)](https://arxiv.org/abs/2506.03481) 🏷️ **[方法 + 基准 + 数据]**
  - **聚焦**: **指称表达式引导的图像编辑** - 通过文本指称表达式（如"左边的红苹果"）进行精确对象级编辑
  - **数据合成方法** - **GPT-4o、定位与FlowChef的可扩展合成Pipeline**:
    - **核心创新**: 利用**可扩展合成数据生成**的少样本学习方法，超越百万级基线
    - **3阶段自动化Pipeline**:
      1. **指令生成（基于GPT-4o）**:
         - 输入: 源图像 + 对象标注（来自RefCOCO/RefCOCO+/RefCOCOg）
         - **GPT-4o**生成涉及指称表达式的多样化编辑指令
         - 指令类型: 添加、删除、替换、重新着色、调整大小、重新定位对象
         - 强调**空间关系**（如"笔记本电脑右侧的瓶子"）
      2. **定位与分割**:
         - **Grounded Segment Anything (Grounded-SAM)**定位被指称对象
         - 为编辑目标生成精确掩码
         - 通过上下文推理处理模糊指称
      3. **使用FlowChef的受控编辑**:
         - **FlowChef**: 基于流的合成，实现自然对象插入/修改
         - 执行编辑的同时保留背景一致性
         - 质量过滤: 基于CLIP的指令遵循验证
    - **关键发现**: 仅**2万合成三元组**即可使模型超越在**>100万样本**上训练的基线
  - **数据规模**: 
    - **RefEdit训练数据**: 2万高质量合成编辑三元组
    - **植根于RefCOCO**: 基于已建立的指称表达式数据集
    - **指令多样性**: 12种编辑类型 × 多样化空间/属性变体
  - **基准**: **RefEdit-Bench**
    - **基础**: RefCOCO测试图像配专家标注的编辑任务
    - **重点**: 评估编辑中指称表达式理解的精确性
    - **指标**: 对象定位准确率 + 编辑质量（CLIP-Sim、FID、人工评估）
  - **模型**: **RefEdit模型**
    - **架构**: 基于扩散的编辑模型，条件化于指称表达式
    - **训练**: 在2万合成数据上微调
  - **实验结果**: 
    - **RefEdit-Bench**: 尽管训练数据少**50倍**，仍超越MagicBrush、InstructPix2Pix
    - **指称精度**: **91.2%对象定位准确率** vs. 仅指令基线的**67.3%**
    - **少样本优越性**: 证明数据质量 > 数量范式
  - **发布时间**: arXiv 2025年6月 (v1: 2025年6月)
  - **机构**: 亚利桑那州立大学
  - **开源**: ✅ RefEdit-Bench、模型、2万训练数据 - 论文中提供详情
  - **交叉引用**: 另见[典型多模态数据集](#-典型多模态数据集)

- **📄 Referring Image Editing / RefCOCO-Edit** [(CVPR 2024论文)](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Referring_Image_Editing_Object-level_Image_Editing_via_Referring_Expressions_CVPR_2024_paper.html) 🏷️ **[任务定义 + 方法 + 早期基准]** - **CVPR 2024**
  - **聚焦**: **首次系统性表述指称图像编辑（RIE）**作为对象级生成任务
  - **核心贡献**: 
    - **任务定义**: 将RIE形式化为通过指称表达式识别特定对象的编辑
    - **ReferDiffusion框架**: 为RIE定制的扩散架构
    - **RefCOCO-Edit数据集**: 源自RefCOCO的早期基准
  - **数据合成方法** - **Paint-by-Example + Blended Latent Diffusion**:
    - **Pipeline**:
      1. **来源**: 带指称表达式标注的RefCOCO图像
      2. **参考图像收集**: 收集目标属性的示例编辑图像
      3. **掩码引导合成**:
         - **Paint-by-Example**: 基于示例的对象属性修复
         - **Blended Latent Diffusion**: 编辑区域与原始图像的平滑混合
      4. **质量控制**: 人工过滤 + 自动一致性检查
    - **规模**: 小规模基准（未指定确切大小，典型早期基准规模~500-2K样本）
  - **RefCOCO-Edit数据集**:
    - **组件**: 图像、带指称表达式的编辑提示、源对象分割掩码、参考编辑图像
    - **编辑类型**: 由指称表达式引导的对象属性变化（颜色、纹理、风格）
  - **ReferDiffusion模型**:
    - **架构**: 双分支扩散模型
      - **指称分支**: 编码指称表达式 + 定位目标对象
      - **编辑分支**: 对掩码区域应用条件扩散
    - **创新**: 首次将指称表达式定位集成到扩散编辑中
  - **实验结果**:
    - 在RefCOCO-Edit上建立基线性能
    - 证明对对象特定任务优于通用指令引导编辑器
  - **重要意义**: 
    - **开创性工作**: 首次将RIE作为独特任务系统性处理
    - **早期基准**: RefCOCO-Edit作为基础评估数据集
    - **方法论蓝图**: 影响后续工作（RefEdit、ImgEdit）
  - **发布时间**: CVPR 2024
  - **机构**: 学术研究（见CVPR论文）
  - **开源**: ✅ RefCOCO-Edit数据集、ReferDiffusion代码（查看CVPR补充材料）
  - **交叉引用**: 另见[基准数据集](#-基准数据集)了解RefCOCO-Edit详情

---

### 🧩 组合性/偏好导向合成

该类别专注于**用于增强组合理解的合成数据生成**。这些方法使用受控数据合成来提升模型理解复杂组合关系（如属性绑定、空间关系、计数）和与人类偏好对齐的能力。

- **📄 SPARCL** [(arXiv 2503.01167)](https://arxiv.org/abs/2503.01167) 🏷️ **[方法 + 合成数据]** - **CVPR 2025**
  - **聚焦**: **通过具有细微变化的多模态合成数据增强VLM组合理解**
  - **数据合成方法** - **真实图像特征注入快速T2I模型 + 自适应边距损失**:
    - **核心创新**: 生成正/负标注对配**视觉基础图像**，训练VLM进行细粒度组合区分
    - **3阶段Pipeline**:
      1. **标注对生成（基于LLM）**:
         - 使用**LLM**（如GPT-4）生成具有细微组合变化的正/负标注对
         - **正标注**: 准确描述图像内容
         - **负标注**: 在保持语义相似性的同时引入组合错误
         - **变化类型**:
           - **属性绑定**: "红苹果和绿香蕉" → "绿苹果和红香蕉"
           - **对象计数**: "三只狗" → "两只狗"
           - **空间关系**: "猫在狗左边" → "狗在猫左边"
           - **动作归属**: "女人拿伞" → "男人拿伞"
      2. **真实特征注入的图像合成**:
         - **挑战**: 纯文生图模型难以实现精确组合控制
         - **解决方案**: 将**真实图像特征**注入快速T2I模型（如SDXL-Turbo）
         - **方法**:
           - 从**真实参考图像**（如COCO、Visual Genome）提取特征
           - **特征注入**: 同时基于文本标注+真实图像特征条件化T2I生成
           - 确保生成图像忠实反映标注中的组合细节
         - **速度**: SDXL-Turbo实现快速生成（4-8步 vs. 标准扩散的50+步）
      3. **用于域对齐的风格迁移**:
         - 对合成图像应用**风格迁移**以匹配真实图像分布
         - 减少合成训练数据与真实测试图像之间的域差距
         - 使用神经风格迁移或轻量级风格适配器
    - **自适应边距损失**:
      - **问题**: 对比学习中的固定边距对所有负样本一视同仁
      - **解决方案**: 基于标注相似度的**自适应边距**
      - **公式**: 边距与正/负标注间文本相似度成反比
      - **效果**: 更难的负样本（更相似的标注）获得更小边距 → 模型学习更精细的区分
  - **数据规模**: 
    - **SPARCL数据集**: 大规模合成组合数据（未指定确切规模，可能10万-100万对）
    - **标注对**: 每张图像关联正标注 + 多个负变体
    - **组合类别**: 涵盖4+种主要组合推理类型
  - **训练策略**:
    - **高效微调**: 使用**LoRA适配器**进行参数高效训练
    - **对比学习**: 训练VLM区分正/负标注-图像对
    - **自适应边距**: 基于标注相似度动态调整边距
  - **实验结果**: 
    - **组合基准**: 在Winoground、ARO、CREPE、SugarCrepe上显著提升
      - **Winoground**: 超越CLIP基线+12.3%
      - **ARO（属性绑定）**: +8.7%
      - **SugarCrepe（困难负样本）**: +10.1%
    - **泛化**: 在标准VQA任务上保持强性能（无退化）
    - **数据效率**: 用中等规模合成数据取得增益（无需billion级数据集）
  - **关键发现**:
    - **真实特征注入至关重要**: 纯T2I生成在细粒度组合控制上失败
    - **自适应边距有效性**: 超越固定边距对比学习**+4.2%**
    - **合成数据充分性**: 目标合成数据比扩大真实数据更有效
  - **发布时间**: CVPR 2025 | arXiv 2025年3月 (v2: 2025年3月)
  - **机构**: 未明确说明（学术研究）
  - **开源**: ✅ 代码、合成数据、LoRA适配器 - 论文中提供发布详情
  - **重要意义**: 
    - **解决VLM弱点**: 针对已知的组合理解局限
    - **高效合成**: 快速T2I + LoRA实现可扩展、成本效益高的数据生成
    - **方法论创新**: 真实特征注入 + 自适应边距为组合数据合成提供蓝图

---

### 🧪 交错图文·连贯性与一致性

该类别专注于**高质量交错图文数据构建**，强调**连贯性（逻辑流程）、一致性（事实准确性）和对齐（图文相关性）**。与简单图文对不同，这些方法策划或合成具有叙事连贯性的多图像文档，适合训练模型进行长上下文多模态理解和生成。

- **📄 CoMM** [(arXiv 2406.10462)](https://arxiv.org/abs/2406.10462) 🏷️ **[方法 + 数据 + 基准]** - **CVPR 2025**
  - **聚焦**: **连贯的交错图文数据集**，配多视角质量过滤和新颖评估任务
  - **数据筛选方法** - **多视角过滤策略 + 质量评估框架**:
    - **核心创新**: 非纯合成，而是使用**跨三个维度的多模型过滤**对网络抓取的交错数据进行**系统性筛选和质量增强**
    - **3视角过滤Pipeline**:
      1. **文本序列过滤（连贯性）**:
         - **目标**: 确保文本序列的逻辑流程和叙事连贯性
         - **方法**: 
           - 使用**基于LLM的连贯性评分**（如GPT-3.5/4）评估文本可读性和逻辑进展
           - 检测并删除具有以下问题的文档:
             - 话题突变
             - 句子不连贯
             - 语法结构差
         - **阈值**: 校准连贯性分数 ≥ 0.75（0-1刻度）
      2. **图像序列过滤（一致性）**:
         - **目标**: 保持同一文档中图像序列的视觉一致性和相关性
         - **方法**: 
           - **基于CLIP的图像相似度**: 测量连续图像间的视觉连贯性
           - **对象/场景一致性**: 使用检测模型（YOLO、DINO）验证图像间一致的实体
           - **美学质量**: 过滤低质量、模糊或重水印图像
         - **标准**: 删除具有以下问题的序列:
           - 图像间相似度极低（< 0.3 CLIP分数）
           - 单一文档内风格/域剧烈变化
           - 大部分图像未通过质量检查
      3. **图文对齐过滤（相关性）**:
         - **目标**: 确保图像与周围文本之间的紧密对齐
         - **方法**: 
           - **跨模态检索验证**: 对每张图像，验证其在针对周围文本段落检索时排名靠前
           - **基于CLIP的图文匹配**: 计算图像与相邻文本之间的对齐分数
           - **标注一致性检查**: 使用VLM（如BLIP-2）为图像生成标注，验证与文档文本的语义匹配
         - **过滤**: 删除对齐分数 < 0.5的图文对
    - **多模型共识**:
      - 组合多个模型（CLIP、BLIP-2、GPT-4、定制分类器）的分数
      - 要求跨模型一致以避免单模型偏差
      - **集成策略**: 使用校准阈值的加权平均
    - **源数据**: 
      - **网络爬取**: CommonCrawl、维基百科、教育网站
      - **初始规模**: 过滤前约200万原始文档
      - **过滤后规模**: 22.7万高质量连贯文档
  - **数据规模**: 
    - **CoMM数据集**: 22.7万文档、228万图像、1.39亿文本token
    - **平均文档长度**: 约611 token、每文档约10张图像
    - **领域**: 新闻文章、教程、教育内容、故事叙事
  - **新颖评估任务（提出4项任务）**:
    1. **连贯图像生成**: 给定文本大纲生成连贯图像序列
    2. **连贯文本生成**: 给定图像序列生成叙事文本
    3. **连贯性评估**: 评估交错文档的连贯性质量
    4. **长上下文跨模态检索**: 在多图像文档中检索相关图像/文本
  - **基准构建**:
    - **测试集**: 5K高质量留出文档
    - **人工标注**: 专家对连贯性、一致性、对齐的评分（3分制）
    - **自动指标**: 基于CLIP、基于LLM和定制连贯性指标
  - **实验结果**: 
    - **使用CoMM预训练**: 在CoMM上训练的模型在交错理解任务上比在非过滤数据上训练的模型提升**+8.3%**
    - **连贯性指标**: CoMM过滤数据得分**0.82连贯性** vs. 原始网络数据的**0.61**
    - **对齐质量**: **87.5%**图文对齐准确率 vs. 未过滤数据的**62.3%**
    - **泛化**: 强迁移到下游任务（长文本VQA、多图像推理）
  - **消融研究**:
    - **每项过滤的贡献**: 
      - 文本过滤: +3.1%连贯性
      - 图像过滤: +2.7%一致性
      - 对齐过滤: +4.2%相关性
    - **多模型共识**: 超越单模型过滤**+5.8%**
  - **发布时间**: CVPR 2025 | arXiv 2024年6月 (v3: 2024年6月)
  - **机构**: 未明确说明（学术研究）
  - **开源**: ✅ CoMM数据集（22.7万文档）、评估基准（5K测试集）、过滤代码、评估指标 - 论文中提供发布详情
  - **交叉引用**: 另见[典型多模态数据集](#-典型多模态数据集)了解数据集详情
  - **重要意义**: 
    - **质量重于数量**: 证明严格过滤 > 原始规模对交错数据的重要性
    - **多维质量**: 首个系统性同时解决连贯性、一致性和对齐问题
    - **方法论贡献**: 多视角过滤框架可应用于其他网络规模筛选任务
    - **基准创新**: 专门为评估交错数据质量提出新任务

---

### 图像不变文本增强

这类方法保持原始图像不变，通过各种技术丰富和改进配对文本质量。**这是目前最主流的多模态数据合成范式。**

> **注意**: 仅收录明确描述数据合成/生成方法的论文，并标注具体的合成组件。

#### 🔬 基于大模型的文本生成

> **核心思想**: 使用强大的VLMs（如GPT-4V）或LLMs（如GPT-4）为图像生成更高质量的captions/对话数据

- **📄 ShareGPT4V** [(arxiv 2311.12793)](https://arxiv.org/abs/2311.12793)
  - **数据合成方法**（Section 3.1）:
    - 使用**GPT-4V**为100K图像生成高质量captions
    - Prompt设计: 要求详细描述（比原始captions详细3-5倍）
    - 图像来源: 从COCO、SAM、LAION等精选图像
  - **数据规模**: 100K高质量captions
  - **开源**: ✅ [Dataset](https://huggingface.co/datasets/Lin-Chen/ShareGPT4V) | [Code](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V)

- **📄 SVIT** [(arxiv 2307.04087)](https://arxiv.org/abs/2307.04087)
  - **数据合成方法**（Section 3.2）:
    - 使用**GPT-4**生成大规模视觉指令数据
    - 4.2M对话 + 1.6M复杂推理
    - 基于多个数据集的captions（COCO, VG, CC3M等）
  - **数据规模**: 5.8M指令数据
  - **开源**: ✅ [Dataset](https://huggingface.co/datasets/BAAI/SVIT)

- **📄 CapsFusion** [(arxiv 2310.20550)](https://arxiv.org/abs/2310.20550)
  - **数据合成方法**（Section 3）:
    - **融合多个captioning模型的输出**（BLIP-2, CoCa, GPT-4V等）
    - 使用加权融合策略，结合不同模型的优势
    - 为DataComp-1B重新生成captions
  - **数据规模**: Billion级重新captioning
  - **开源**: ✅ [Code](https://github.com/baaivision/CapsFusion)

- **📄 Image Textualization（图像文本化）** [(arxiv 2406.07502)](https://arxiv.org/abs/2406.07502)
  - **数据合成方法** - **详细图像描述自动生成框架**：
    - **核心创新**：结合**MLLM理解能力**与**视觉专家感知能力**，最大化将视觉信息转换为文本
    - **三阶段管道**：
      - **阶段1 - 整体文本化**： 
        - 利用MLLM创建**参考描述**
        - 为视觉信息和语言表达提供基础结构
        - 包含"骨架"，但缺乏细节且含有幻觉
      - **阶段2 - 视觉细节文本化**：
        - 使用视觉专家模型（目标检测、密集描述、实例分割）提取**细粒度物体级信息**
        - 从图像侧提取多个细节（用高分辨率图像和物体级标注训练）
        - 识别并过滤参考描述中的幻觉
        - 将感知结果转换为文本格式
      - **阶段3 - 文本化重述**：
        - 使用LLM整合视觉细节与参考描述
        - 生成既丰富细节又无幻觉的最终高质量描述
    - **核心优势**：
      - 解决MLLM弱点：视觉幻觉问题和缺乏细粒度细节
      - 视觉专家提供精确感知（高分辨率训练、物体级标注）
      - MLLM提供整体理解能力
  - **数据规模**：创建图像描述数据集（规模因应用而异）
  - **实验结果**： 
    - 在IT策划描述上训练的LLaVA-7B生成**更丰富的图像描述**
    - 大幅增加输出长度和细节，**减少幻觉**
    - 多个benchmark上的全面评估验证了描述质量
  - **发布时间**：arXiv 2024年6月
  - **机构**：HKUST、武汉大学、浙江大学、UIUC
  - **开源**：✅ [代码](https://github.com/sterzhang/image-textualization/) | [数据集](https://huggingface.co/datasets/Sterzhang/image-textualization/)

#### 🤖 基于VLM/LLM的合成文本生成

> 以下论文明确描述了如何使用大模型为图像生成合成captions/对话

- **📄 LLaVAR** [(arxiv 2306.17107)](https://arxiv.org/abs/2306.17107)
  - **数据合成方法**（Section 3.2）:
    - 针对富文本图像（文档、海报、图表等）
    - 使用**GPT-4**基于OCR结果生成指令Q&A对
    - 生成"理解+推理"类型问题（不只是读文本）
  - **数据规模**: 422K指令数据
  - **开源**: ✅ [Code](https://github.com/SALT-NLP/LLaVAR)

- **📄 ALLaVA** [(arxiv 2402.11684)](https://arxiv.org/abs/2402.11684)
  - **数据合成方法**（Abstract明确描述）:
    - **Captioning-then-QA范式**: 两阶段数据生成
    - 使用**GPT-4V**生成两类数据:
      a) **细粒度图像标注**用于视觉-语言对齐
      b) **复杂推理的视觉问答对**用于视觉指令微调
    - 完整的数据生成pipeline，利用强大的专有模型合成高质量数据
  - **数据规模**: 1.3M样本
  - **实验结果**: 在4B规模模型中达到竞争力性能，甚至在部分benchmark上与7B/13B模型相当
  - **开源**: ✅ 数据集开源（论文中提到）

- **📄 COGS** [(arxiv 2510.15040)](https://arxiv.org/abs/2510.15040)
  - **数据合成方法**（Abstract明确描述）:
    - **Composition-Grounded Instruction Synthesis（组合式指令合成）**
    - 从**少量种子问题**开始，通过分解-重组生成大规模数据:
      a) **分解**: 将种子问题分解为原始感知和推理因子
      b) **重组**: 将因子与新图像系统地重组
      c) **生成**: 创建大量合成问答对，每个配有子问题和中间答案
    - 支持因子级过程奖励的强化学习
  - **应用领域**: 图表、网页、渲染文档等人工图像领域
  - **实验结果**: 在未见问题上大幅提升，推理密集型和组合性问题提升最大，跨数据集迁移效果好
  - **发布时间**: arXiv 2025年10月
  - **机构**: MIT、IBM Research等

#### 🛠️ 工具辅助标注生成（用于数据合成）

> 以下工具常用于数据合成pipeline

- **📄 All-Seeing Project** [(arxiv 2308.01907)](https://arxiv.org/abs/2308.01907)
  - **数据合成方法**（Section 3）:
    - 使用SAM、RAM、Tag2Text等工具自动生成多层级标注
    - Pipeline: 图像 → 分割+Tags → 区域描述 → 指令数据
    - 构建AS-1B数据集（12亿region-text对）
  - **这是真正的数据合成**: 使用工具组合生成新标注
  - **开源**: ✅ [Dataset](https://huggingface.co/datasets/OpenGVLab/AS-V2) | [Code](https://github.com/OpenGVLab/all-seeing)


---

## 🧠 跨领域方法论启发

> **说明**: 本节收录来自相邻领域（如LLM推理、数学数据筛选）的有影响力工作，这些工作的**数据筛选与质量评估方法论**为多模态场景提供了有价值的可迁移框架。虽然不是直接的VLM论文，但它们在数据质量评估、混合优化和高效过滤管道方面的系统化方法为多模态数据从业者提供了重要启发。

### 📚 基础模型中期训练与数据筛选

<details>
<summary><b>OctoThinker: 中期训练激励强化学习扩展</b> - 系统化的数据筛选、质量评估与混合优化</summary>

**论文**: [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)

**机构**: 上海交通大学 GAIR Lab (Wang等)

**发布时间**: arXiv 2025年6月

**领域**: 数学推理 / LLM中期训练

**🔬 为何与VLM数据筛选相关？**

**核心贡献**: 本文**并非关于数据合成/生成**，而是关于**系统化的数据质量评估、大规模过滤/筛选和混合优化**。虽然专注于文本领域的数学推理，OctoThinker提供了一个严谨的实验框架，用于从现有语料中评估和选择高质量数据——这些方法高度可迁移到多模态场景，特别是从噪声网络源中筛选十亿级数据集时。

---

**📊 核心可迁移洞察**:

**1. 两阶段质量评估管道**
- **方法**: LLM标注(0-5分) → 训练高效分类器(FastText) → 大规模过滤(阈值0.4) → 可选LLM精炼
- **关键发现**: 预处理至关重要；阈值0.4平衡质量-数量权衡（前人使用0.9）
- **成果**: MegaMath-Web-Pro-Max (73.8B tokens, 是MegaMath-Web-Pro 13B的5.5倍)
- **VLM迁移**: 构建多模态标注schema以评估视觉-语言对齐质量、信息密度和组合推理复杂度；训练轻量级跨模态判别器（冻结视觉编码器）实现成本可控的十亿级数据筛选，同时保持高保真过滤能力

**2. 系统化数据混合优化**
- **严格测试**: 跨多个维度测试10%、20%、30%、40%比例
- **最优发现**: 30% QA数据（超过后因冗余而收益递减）
- **核心洞察**: *"训练数据与下游任务的分布差距显著影响性能"*
- **VLM迁移**: 建立受控实验框架，系统性变化caption粒度（实体级 vs 场景级 vs 推理级）、数据来源（网页爬取 vs 人工标注 vs 模型合成）和跨模态交互模式；采用网格搜索结合严格下游评估，而非启发式混合

**3. 分布对齐 > 数据规模**
- **观察**: 结构化QA数据集(OpenR1, OMI2)在竞赛风格基准上优于网页来源QA(MegaMath-QA)，原因是分布对齐而非数据体量
- **原则**: 匹配预训练分布到下游任务格式，而非仅匹配领域
- **VLM迁移**: 优先考虑分布感知的数据筛选而非体量最大化——文档理解受益于文本密集的多样排版布局而非自然图像；视觉推理需要多跳关系标注而非描述性captions；领域特定应用要求模态一致且结构对齐的预训练语料

**4. 少量"稳定剂"数据影响巨大**
- **发现**: 少量(1-10%)高质量指令数据解锁其他数据类型的潜力
- **效果**: 稳定训练，减少长格式数据不稳定性，使RL扩展成功
- **VLM迁移**: 引入小比例高质量视觉-语言对齐数据(1-10%)以正则化训练动态，特别是在扩展到密集长文本captions或复杂多图推理场景时；充当分布锚点防止模式坍塌和表征漂移

---

**🎯 总结: VLM数据从业者的关键要点**

| # | 原则 | OctoThinker方法 | VLM应用 |
|---|------|----------------|---------|
| 1 | **质量 > 数量** | 过滤后的MegaMath-Web-Pro-Max >> 原始语料 | 精筛选的高精度数据对在下游迁移中显著优于原始网络规模噪声语料 |
| 2 | **高效过滤** | LLM标注 → FastText分类器 → 规模化 | 可扩展两阶段管道实现十亿级筛选并保持计算效率 |
| 3 | **系统化实验** | 测试10%, 20%, 30%, 40%比例 | 在数据混合超参数空间进行网格搜索与严格消融研究 |
| 4 | **收益递减** | 超过30% QA，冗余有害 | 识别边际效用因分布冗余而递减的饱和点 |
| 5 | **分布对齐** | 匹配下游任务格式 | 数据格式与任务对齐比领域匹配更重要 |
| 6 | **稳定剂数据** | 1-10%指令数据解锁QA潜力 | 小比例正则化数据防止训练不稳定并实现扩展 |
| 7 | **格式意识** | 长CoT = 高能力 + 不稳定性 | 高容量密集输出需要稳定机制以防止模式坍塌 |
| 8 | **两阶段哲学** | 基础 (90%) → 专门化 (10%) | 广泛的分布覆盖后进行针对性能力专门化 |
| 9 | **预处理很重要** | 对分类器性能至关重要 | 系统化归一化和伪影消除对判别器泛化能力至关重要 |
| 10 | **实证验证** | 阈值在下游任务上验证 | 超参数选择由下游任务性能指导而非代理指标 |

---

**📈 规模与资源**:
- **基础模型**: Llama-3.2-1B/3B/8B
- **中期训练预算**: 最多200B tokens (稳定阶段) + 20B tokens (衰减阶段)
- **最终语料**: MegaMath-Web-Pro-Max (73.8B tokens, 是MegaMath-Web-Pro的5.5倍)
- **消融研究**: 系统化的受控实验，涵盖数据质量、混合比例、QA来源和格式特征
- **性能提升**: 比基础模型提升10-20%，RL后匹配Qwen2.5

**🔗 资源**:
- ✅ **论文**: [arXiv:2506.20512](https://arxiv.org/abs/2506.20512)
- ✅ **代码**: [GitHub - GAIR-NLP/OctoThinker](https://github.com/GAIR-NLP/OctoThinker)
- ✅ **模型**: [HuggingFace - OctoThinker](https://huggingface.co/OctoThinker)
- ✅ **数据集**: MegaMath-Web-Pro-Max (70B+ tokens, 承诺开源)
- ✅ **完全公开**: 论文附录详述所有prompts和方法

**💡 潜在的VLM后续工作**:
- 系统化的视觉-语言数据混合优化
- 高效多模态质量分类器
- 针对特定领域的分布对齐合成caption生成
- 带有专门化分支的两阶段VLM预训练

</details>

---

## 📦 典型多模态数据集

> **说明**: 本节列出有影响力的大规模多模态数据集，这些数据集作为训练视觉-语言模型的基础。它们通常从多个来源筛选而来，代表了重要的数据聚合/整理工作。

### 📦 交错图文数据集

这些数据集包含多图像文档，其中图像与文本段落交错排列，模拟真实世界文档（文章、教程、故事）。用于训练长上下文多模态理解和交错图文生成。

| 数据集 | 规模 | 描述 | 链接 |
|--------|------|------|------|
| **OmniCorpus** | 8.6B图像<br/>1,696B文本token | 统一多语多源数据引擎构建的大规模交错图文语料，强调可扩展构建与多语言覆盖 | [📄 论文](https://arxiv.org/abs/2506.03448) |
| **OBELICS** | 141M文档<br/>353M图像<br/>115B token | 从CommonCrawl提取的网络规模交错数据，完整公开过滤流程 | [📄 论文（NeurIPS 2023 D&B）](https://arxiv.org/abs/2306.16527) |
| **MMC4** | 101.2M文档<br/>571M图像<br/>43B token | 使用CLIP特征和线性指派算法对齐图文，目前提供部分重新托管切分 | [📄 论文（NeurIPS 2023 D&B）](https://arxiv.org/abs/2304.06939) |
| **CoMM** | 227K文档<br/>2.28M图像<br/>139M token | **高质量连贯的交错数据集**，通过多视角过滤（连贯性、一致性、对齐）筛选；配新颖评估任务 | [📄 论文（CVPR 2025）](https://arxiv.org/abs/2406.10462) \| 参见[方法栏](#-交错图文连贯性与一致性) |

### 📊 领域特定与知识导向数据集

这些数据集关注特定领域（如遥感）或知识型任务（如实体集扩展），提供领域专用的多模态训练资源。

| 数据集 | 规模 | 领域/任务 | 描述 | 链接 |
|--------|------|-----------|------|------|
| **MMM-RS** | 2.1M图文对 | 遥感 | **遥感多模态数据集**，用于文生图任务。特点：**多模态**（光学、SAR）、**多GSD**（地面采样距离）、**多场景**（雾、雪、低光）覆盖。标准化9个公开遥感数据集，配自动captioning与合成场景增强。 | [📄 论文（NeurIPS 2024 D&B）](https://arxiv.org/abs/2307.14878) |
| **MESED** | 14,489个实体<br/>434,675图文对 | 知识/实体 | **多模态实体集扩展**数据集，配**细粒度语义类别**和**困难负样本**。面向知识/实体中心任务设计。包含基线模型MultiExpan。 | [📄 论文（AAAI 2024）](https://arxiv.org/abs/2406.08418) |

### 🎨 图像编辑数据集

这些数据集专注于指令引导的图像编辑，提供（源图像、编辑指令、编辑后图像）三元组。虽然许多是"方法+数据"工作（主要列在[方法栏](#-图像编辑方法数据)），但由于其作为重要编辑基准的地位，在此提供快速索引。

| 数据集 | 规模 | 编辑类型 | 描述 | 链接 |
|--------|------|----------|------|------|
| **ByteMorph-6M** | 600万三元组 | 非刚性运动 | **首个大规模非刚性运动编辑数据集**：相机运动、物体形变、人体关节运动、人物-物体交互；配ByteMorph-Bench（613测试样本） | [📄 论文](https://arxiv.org/abs/2506.03107) \| 参见[大厂报告](#字节跳动---bytemorph) & [方法栏](#-图像编辑方法数据) |
| **ImgEdit** | 120万三元组<br/>（95万单轮 + 25万多轮） | 统一编辑 | **统一图像编辑数据集**：8大编辑类别（对象添加/删除/替换、属性修改、背景变化、风格迁移、空间重排、多对象编辑）；多轮编辑配身份一致性；配ImgEdit-Bench（1K测试样本） | [📄 论文](https://arxiv.org/abs/2505.20275) \| 参见[方法栏](#-图像编辑方法数据) |
| **RefEdit** | 20K三元组 | 指称编辑 | **指称表达式引导的编辑数据集**：精确对象级编辑（如"左边的红苹果"）；少样本高质量数据优于百万级基线；配RefEdit-Bench | [📄 论文](https://arxiv.org/abs/2506.03481) \| 参见[方法栏](#-图像编辑方法数据) |
| **RefCOCO-Edit** | 早期小规模基准<br/>（~500-2K样本） | 指称编辑 | **早期指称图像编辑基准**：首次系统性定义RIE任务；源自RefCOCO，使用Paint-by-Example + Blended Latent Diffusion合成 | [📄 论文（CVPR 2024）](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Referring_Image_Editing_Object-level_Image_Editing_via_Referring_Expressions_CVPR_2024_paper.html) \| 参见[方法栏](#-图像编辑方法数据) |

### 🎓 大规模通用训练数据集

| 数据集 | 规模 | 描述 | 链接 |
|--------|------|------|------|
| **FineVision** | 2430万样本<br/>1730万图像 | 从200+数据源聚合的综合性多模态数据集，涵盖9大类别：通用VQA、OCR、图表/表格、文档、Grounding、数学、科学、视觉中心和世界知识 | [🤗 HuggingFace](https://huggingface.co/datasets/HuggingFaceM4/FineVision) \| [📄 论文](https://arxiv.org/abs/2510.17269) |
| **LLaVA-OneVision** | 约400万样本 | 统一的高质量数据集，涵盖单图像、多图像和视频场景 | [🤗 HuggingFace](https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data) \| [📄 论文](https://arxiv.org/abs/2408.03326) |
| **PixMo** | 多个子集 | 用于训练开放视觉-语言模型的数据集套件（PixMo-Cap、PixMo-AskModelAnything等） | [🤗 HuggingFace](https://huggingface.co/collections/allenai/pixmo-674746ea613028006285687b) \| [📄 论文](https://arxiv.org/abs/2409.17146) |
| **MAmmoTH-VL** | 1200万样本 | 用于增强MLLM推理能力的大规模多模态指令微调数据集 | [🤗 HuggingFace](https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M) \| [📄 论文](https://arxiv.org/abs/2412.05237) |

---

## 📊 基准数据集

### 🎯 指令跟随基准

| 基准 | 描述 | 链接 |
|------|------|------|
| MMBench | 综合多模态能力评估 | [GitHub](https://github.com/open-compass/MMBench) |
| SEED-Bench | 层次化多模态理解评估 | [GitHub](https://github.com/AILab-CVC/SEED-Bench) |
| LLaVA-Bench | 评估野外场景下的对话、详细描述和复杂推理能力 | [🤗 HuggingFace](https://huggingface.co/datasets/lmms-lab/llava-bench-in-the-wild) |

### 📝 任务特定基准

| 基准 | 描述 | 链接 |
|------|------|------|
| VQAv2 | 视觉问答 | [官网](https://visualqa.org/) |
| GQA | 组合推理评估 | [官网](https://cs.stanford.edu/people/dorarad/gqa/) |
| MMMU | 大规模多学科多模态理解与推理 | [官网](https://mmmu-benchmark.github.io/) |

---

## 🎓 资源

### 💡 重要启发性工作

| 论文 | 年份 | 重要性 | 链接 |
|------|------|--------|------|
| Magpie: Alignment Data Synthesis from Scratch | 2024 | Oasis的核心灵感来源，文本领域的自对齐指令生成方法 | [arXiv](https://arxiv.org/abs/2406.08464) |

> ⚠️ **注意**: Magpie本身是纯文本LLM的方法，不是多模态数据合成

### 📖 综述论文（涵盖数据合成）

| 论文 | 年份 | 重点内容 | 链接 |
|------|------|----------|------|
| A Survey on Multimodal Large Language Models | 2023 | Section 3.2专门讨论训练数据构建 | [arXiv](https://arxiv.org/abs/2306.13549) |
| Multimodal Foundation Models: From Specialists to General-Purpose Assistants | 2024 | 包含数据收集和合成方法综述 | [arXiv](https://arxiv.org/abs/2309.10020) |

### 🔗 相关Awesome列表

| 列表 | 描述 | 链接 |
|------|------|------|
| Awesome Data for LLM | LLM数据工程（文本领域） | [GitHub](https://github.com/weAIDB/awesome-data-llm) |
| Awesome Scientific Datasets and LLMs | 科学领域数据集 | [GitHub](https://github.com/open-sciencelab/Awesome-Scientific-Datasets-and-LLMs) |
| Awesome Multimodal ML | 综合多模态ML资源 | [GitHub](https://github.com/pliang279/awesome-multimodal-ml) |

### 📝 重要博客

| 博客 | 描述 | 链接 |
|------|------|------|
| LLaVA Blog | LLaVA系列数据生成方法详解 | [官网](https://llava-vl.github.io/) |
| HuggingFace Blog | 多模态数据和模型教程 | [官网](https://huggingface.co/blog) |

---

## 🤝 贡献指南

我们欢迎各种形式的贡献！包括但不限于：

- 🆕 添加新论文、工具或数据集
- 📝 改进现有条目的描述
- 🐛 修正错误或过时信息
- 💡 提出改进建议

### 如何贡献

1. Fork本仓库
2. 创建特性分支（`git checkout -b feature/AmazingFeature`）
3. 提交更改（`git commit -m 'Add some AmazingFeature'`）
4. 推送到分支（`git push origin feature/AmazingFeature`）
5. 开启Pull Request

### 贡献指南

- 添加新条目时，请确保包含：
  - 📄 论文/工具名称和链接
  - 📝 清晰简洁的描述（1-2句话）
  - 🔗 相关链接（代码、数据集、博客等）
- 保持条目按字母顺序或重要性排序
- 确保链接有效且指向官方资源
- 使用中文进行描述

---

## ⭐ Star历史

如果这个项目对您有帮助，请给我们一个Star ⭐️！

[![Star History Chart](https://api.star-history.com/svg?repos=opendatalab-raiser/awesome-multimodal-data-recipe&type=Date)](https://star-history.com/#opendatalab-raiser/awesome-multimodal-data-recipe&Date)

---

## 📄 许可证

本项目采用[CC0-1.0 License](LICENSE)。

---

## 🙏 致谢

感谢所有为多模态数据合成领域做出贡献的研究人员和工程师！

特别感谢以下项目的启发：
- [awesome-data-llm](https://github.com/weAIDB/awesome-data-llm)
- [Awesome-Scientific-Datasets-and-LLMs](https://github.com/open-sciencelab/Awesome-Scientific-Datasets-and-LLMs)

---

<p align="center">
  由多模态研究社区用 ❤️ 制作
</p>

